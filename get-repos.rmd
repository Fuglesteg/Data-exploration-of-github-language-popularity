---
title: Gather github dataset
date: "`r Sys.Date()`"
output: pdf_document
---

# Github API
In our project we have decided to create our own dataset by using the GitHub GraphQL API.
The github GraphQL API allows us to query github for repositories and retrieve specific
data from those repositories. The API does however have many limitations and restrictions
that we will have to circumvent.

## Pagination limitation
The API can retrieve a max number of 100 items per request, into a "page".
However the API has a built in way of overcoming this limitation, you can ask
the API for an "endCursor", which you can then use in your query to specify
results **after** the cursor

## Max results limitation
Even with overcoming pagination, the API restricts the total number of results
to 1000. We overcome this by using the stargazerCount value to construct a new query.
This works because our results are sorted using stars, so querying for
repositories that have less stars than the repository at the end of our last search
effectively means that we are resuming the search.

## Stars query limitation
Providing specific ranges over large values in the query parameter results in very inaccurate
results. Meaning that if we for example have a query that specifies repositories with less than
20 000 stars, the response could get confused and return repositories starting at below 6 000
stars. To circumvent this we use the range syntax (``` stars:100..20000```) to provide a smaller
range.

## Timeout limitation
While the GitHub API allows 100 results per page, on complex queries the processing time
could be so significant as to cause a request timeout where you would get an error
message and no results. To circumvent this we have set the results per page
to 10 instead of the max value of 100.

\newpage

# Gathering data
This is the logic for retrieving data from the GitHub API, it takes into
consideration the previously mentioned limitations.
It loops over the specified amount of times (See: "total_runs") and 
constructs a query using some dynamic and static variables.
It uses pagination until it encounters no more pages where it will then
construct a new query see [max results limitation](#Max results limitation).

Note that you require an authenticaiton key which you can get from the developer settings
panel in GitHub.

```{r setup, include = FALSE}
load("repo_edges")
```

```{r, eval = FALSE}
results_per_page <- 10
total_results <- 5000
total_runs <- total_results / results_per_page

library("httr")

page_key_query <- ""
stars_query <- '"stars:100..300000"'

# NOTE: Loads the API key from a local file, 
# this key is specific to each user and is required to run the request
key <- readLines("key.secret")

repo_edges <- c()

for (i in 1:total_runs) {
    search_query <- paste(
        'query: ', stars_query,
        ', type: REPOSITORY, first: ',
        results_per_page, ',',
        page_key_query, sep = "")

    query <- paste(
    '
    query {
        search(', search_query, '){
            repositoryCount
                edges {
                    node {
                        ... on Repository {
                            stargazerCount
                            nameWithOwner
                            primaryLanguage {
                                name
                            }
                            issues {totalCount}
                            pullRequests {totalCount}
                            labels(first: 100, query: "bug") {
                                totalCount
                                nodes {
                                    name
                                    pullRequests {
                                        totalCount
                                    }
                                    issues {
                                        totalCount
                                    }
                                }
                            }
                        }
                    }
                }
            pageInfo {
                endCursor
                hasNextPage
            }
        }
    }',
    sep = '')
    body <- list(query = query)
    response <- POST("https://api.github.com/graphql", body = body, encode = "json",
        add_headers(Authorization = paste("Bearer", key)))
    data <- content(response)$data
    repo_edges <- append(repo_edges, data$search$edges)
    if (is.null(data)) {
        break
    }

    if (!data$search$pageInfo$hasNextPage) {
        print("No next page")
        print("Starting new query")
        stars_query <- paste('"stars:100..',
                             data$search$edges[[results_per_page]]$node$stargazerCount - 1, 
                             '"', sep = '')
        print(stars_query)
        page_key_query <- ""
    } else {
        page_key_query <- paste(', after: "', data$search$pageInfo$endCursor, '"', sep = "")
        print(paste(i, "/", total_runs))
    }
}
```

\newpage

# Formatting the data
The data is formatted into a data frame.
Note that we loop over labels again to check if they actually contain the word bug.
And if multiple labels contain the word bug their issues and pull requests will be
summed and stored in a column in the data frame.

```{r}
repos <- data.frame()
for (edge in repo_edges) {
    repo <- edge$node
    bug_labels <- Filter(function(label) grepl("bug", label$name, ignore.case = TRUE),
                         repo$labels$nodes)
    bug_issues <- 0
    bug_pull_requests <- 0

    for (label in bug_labels) {
        bug_issues <- bug_issues + label$issues$totalCount
        bug_pull_requests <- bug_pull_requests + label$pullRequests$totalCount
    }

    if (is.null(repo$primaryLanguage))
        language <- NULL
    else
        language <- repo$primaryLanguage$name

    repos <- rbind(repos,
        data.frame(name = repo$nameWithOwner,
                   stars = repo$stargazerCount,
                   language = I(list(language)),
                   issues = repo$issues$totalCount,
                   pull_requests = repo$pullRequests$totalCount,
                   bug_issues = I(list(bug_issues)),
                   bug_pull_requests = I(list(bug_pull_requests))))
}
```

\newpage

# Filtering data
We are going to work on a subset of the data we have collected.
Not all data is relevant to our problem so we have decided to filter repos based
on the following criteria:

- Repo has no issues labelled as bugs or no pull requests labeled as bugs
- Repo has no primary programming language

Many repositories on GitHub are not actually software projects,
they can be collections of other repositories or projects, typically called
"awesome lists". Or collections of courses or books.
There is also a selection of repositories which do not use issues(or at least issues on GitHub) 
to track bugs. These include older projects which traditionally use a mailing list (e.g Linux).
Using the criteria above we are able to filter out these kinds of repositories
```{r}
repos_filtered <- repos[repos$bug_issues != 0 | repos$bug_pull_requests != 0, ]
repos_filtered <- repos_filtered[!(is.null(repos_filtered$language) |
                        repos_filtered$language == "NULL" |
                        repos_filtered$language == "" |
                        is.na(repos_filtered$language)), ]

nrow(repos) # Rows before filtering
nrow(repos_filtered) # Rows after filtering
```
