---
title: Gather github dataset
date: "`r Sys.Date()`"
output: pdf_document
---

# Data collection

## Github GraphQL API
In our project we have decided to create our own dataset by using the GitHub GraphQL API.
The github GraphQL API allows us to query github for repositories and retrieve specific
data from those repositories. The API does however have many limitations and restrictions
that we will have to circumvent.

### Pagination limitation
The API can retrieve a max number of 100 items per request, into a "page".
However the API has a built in way of overcoming this limitation, you can ask
the API for an "endCursor", which you can then use in your query to specify
results **after** the cursor

### Max results limitation
Even with overcoming pagination, the API restricts the total number of results
to 1000. We overcome this by using the stargazerCount value to construct a new query.
This works because our results are sorted using stars, so querying for
repositories that have less stars than the repository at the end of our last search
effectively means that we are resuming the search.

### Stars query limitation
Providing specific ranges over large values in the query parameter results in very inaccurate
results. Meaning that if we for example have a query that specifies repositories with less than
20 000 stars, the response could get confused and return repositories starting at below 6 000
stars. To circumvent this we use the range syntax (``` stars:100..20000```) to provide a smaller
range.

### Timeout limitation
While the GitHub API allows 100 results per page, on complex queries the processing time
could be so significant as to cause a request timeout where you would get an error
message and no results. To circumvent this we have set the results per page
to 10 instead of the max value of 100.

\newpage

## Data gathering process
This is the logic for retrieving data from the GitHub API, it takes into
consideration the previously mentioned limitations.
It runs for a prespecified amount of times over the specified amount of times
(See: "total_runs") in order to retrieve the requested amount of repositories.
It constructs a query using dynamic and static variables.
It uses pagination until it encounters no more pages where it will then
construct a new query see [max results limitation](#Max results limitation).

Note that you require an authentication key which you can get from the developer settings
panel in GitHub.

We enclose the logic in a function so that we can reuse it later. As
mentioned this paper contains two research questions, we will first gather the
data to answer the first research question RQ1. Later we will use the funciton
again to retrieve the dataset to answer RQ2.
(See: ...)

```{r setup, include = FALSE}
# NOTE: Loads previously collected responses so that requests
# don't need to be run again every time.
load("repo_edges_30_000")
```

```{r get-data, eval = FALSE}

get_repos <- function(n = 10, custom_search_params = "", key = "", results_per_page = 10, body = "") {
    total_runs <- n / results_per_page
    print(paste(total_runs, " runs total."))
    library("httr")
    page_key_query <- ""
    stars_query <- 'stars:100..300000'
    repo_edges <- c()

    i <- 1
    while (i <= total_runs) {
        search_query <- paste(
            'query: "', stars_query, ' ', custom_search_params,
            '", type: REPOSITORY, first: ',
            results_per_page,
            page_key_query, sep = "")

        query <- paste(
            'query {
                search(', search_query, ') {
                    edges {
                        node {
                            ... on Repository { ',
                            body,
                            '}
                        }
                    }
                    pageInfo {
                        endCursor
                        hasNextPage
                    }
                }
            }',
            sep = '')
        request_body <- list(query = query)
        response <-
            POST("https://api.github.com/graphql", body = request_body, encode = "json",
                 add_headers(Authorization = paste("Bearer", key)))
        data <- content(response)$data
        if (is.null(data) && !is.null(content(response)$errors)) {
            print(content(response)$errors)
            print("^ Encountered error, re-running request...")
            next()
        }
        repo_edges <- append(repo_edges, data$search$edges)

        if (!data$search$pageInfo$hasNextPage) {
            print("No next page")
            print("Starting new query")
            stars_query <- paste('stars:100..',
                data$search$edges[[length(data$search$edges)]]$node$stargazerCount - 1,
                sep = '')
            print(stars_query)
            page_key_query <- ""
        } else {
            page_key_query <-
                paste(', after: "', data$search$pageInfo$endCursor, '"', sep = "")
            cat(paste("\r", i, "/", total_runs))
        }
        i <- i + 1
    }
    return(repo_edges)
}

get_repos_for_language_pop <- function(n = 10, key = "", results_per_page = 10) {
    body <- "
    stargazerCount
    nameWithOwner
    primaryLanguage {
        name
    }
    pushedAt
    createdAt
    forkCount
    isInOrganization
    repositoryTopics(first: 100) {
        nodes {
            topic {
                name
            }
        }
    }
    watchers {
        totalCount
    }
    "
    return(get_repos(n, key = key, body = body, results_per_page = results_per_page))
}

get_repos_for_bug_analysis <- function(n = 10, key = "", results_per_page = 10) {
    body <- "
    stargazerCount
    nameWithOwner
    primaryLanguage {
        name
    }
    issues {
        totalCount
    }
    pullRequests {
        totalCount
    }
    labels(first: 100) {
        totalCount
        nodes {
            name
            pullRequests {
                totalCount
            }
            issues {
                totalCount
            }
        }
    }
    createdAt
    forkCount
    isArchived
    isDisabled
    isFork
    isInOrganization
    diskUsage
    isUserConfigurationRepository
    pushedAt
    repositoryTopics(first: 100) {
        nodes {
            topic {
                name
            }
        }
    }
    watchers {
        totalCount
    }
"
    return(get_repos(n, key = key, body = body))
}
repo_lang_edges %>% map(as.data.table) %>% rbindlist(fill = TRUE) -> repo_lang

# NOTE: Loads the API key from a local file,
# this key is specific to each user and is required to run the request
key <- readLines("key.secret")
repo_lang_edges <- get_repos_for_language_pop(100000, key = key, results_per_page = 100)
```

\newpage

# Formatting the data
The data is processed into the appropriate values and transformed into a data frame.
Note that we collapse the topic names into a single comma separated string.

```{r format-data, cache = TRUE}
library(purrr)
format_lang_repo_edges <- function(repo_edges) {
    repos <- data.frame()
    for (edge in repo_edges) {
        repo <- edge$node

        if (is.null(repo$primaryLanguage))
            language <- NULL
        else
            language <- repo$primaryLanguage$name

        # Turn topics into a csv string
        topics <-
            map(repo$repositoryTopics$nodes,
                function(node) return(node$topic$name))
        topics <- paste(topics, collapse = ",")

        repos <- rbind(repos,
            data.frame(
                name = repo$nameWithOwner,
                stars = repo$stargazerCount,
                language = I(list(language)),
                pushedAt = repo$pushedAt,
                createdAt = repo$createdAt,
                forkCount = repo$forkCount,
                isInOrganization = repo$isInOrganization,
                watcher = repo$watcher$totalCount,
                topics = topics
            )
        )
    }
    return(repos)
}
lang_repos <- format_lang_repo_edges(repo_lang_edges)

format_bug_repo_edges <- function(repo_edges) {
    repos <- data.frame()
    for (edge in repo_edges) {
        repo <- edge$node
        bug_labels <-
            Filter(function(label) grepl("(bug|error|conflict|report)", label$name, ignore.case = TRUE),
                   repo$labels$nodes)
        bug_issues <- 0
        bug_pull_requests <- 0
        # TODO: Filter on topics: education, books, list, resource, learning-resources, educational-resources, interview, study, awesome-lists, awesome, lists, resources, roadmap, design, tutorial, tutorials, dotfiles

        for (label in bug_labels) {
            bug_issues <- bug_issues + label$issues$totalCount
            bug_pull_requests <- bug_pull_requests + label$pullRequests$totalCount
        }

        if (is.null(repo$primaryLanguage))
            language <- NULL
        else
            language <- repo$primaryLanguage$name

        repos <- rbind(repos,
                       data.frame(name = repo$nameWithOwner,
                                  stars = repo$stargazerCount,
                                  language = I(list(language)),
                                  issues = repo$issues$totalCount,
                                  pull_requests = repo$pullRequests$totalCount,
                                  bug_issues = I(list(bug_issues)),
                                  bug_pull_requests = I(list(bug_pull_requests))))
    }
    return(repos)
}
```

\newpage

# Cleaning data
We are going to work on a subset of the data we have collected.
Not all data is relevant to our problem so we have decided to filter repos based
on the following criteria:

- Repo has no primary programming language
- Repo has topic of education or similar
- Repo has a primary language which is not a general purpose programming language (Markup, framework or DSL)
- Repo language occurs fewer than 10 times

- RQ2:
    - Another limitation of the GitHub API is that sometimes issues and pull request count is wrong
      so we are filtering on where pull requests or issues count is lower than bug issues and bug PRs
    - Repo has no issues labelled as bugs or no pull requests labeled as bugs

Many repositories on GitHub are not actually software projects,
they can be collections of other repositories or projects, typically called
"awesome lists". Or collections of courses or books.
There is also a selection of repositories which do not use issues(or at least issues on GitHub) 
to track bugs. These include older projects which traditionally use a mailing list (e.g Linux).
Using the criteria above we are able to filter out these kinds of repositories
```{r cleaning-data}
# Remove repos with no bugs
repos_filtered <- repos[repos$bug_issues != 0 | repos$bug_pull_requests != 0, ]

# Remove repos with no primary language
lang_repos_clean <- lang_repos[!(is.null(lang_repos$language) |
    lang_repos$language == "NULL" |
    lang_repos$language == "" |
    is.na(lang_repos$language)), ]

markup_langs <- c(
    "TeX", "AsciiDoc", "HTML",
    "Jupyter Notebook", "FreeMarker", "QML",
    "Roff", "Rich Text Format", "Markdown",
    "Jinja", "Astro", "ASP",
    "Bicep", "Bikeshed", "Haml",
    "Gherkin", "HCL", "Handlebars",
    "Jsonnet", "Liquid", "Mako",
    "Mustache", "Nunjucks", "PlantUML",
    "reStructuredText", "Smarty", "Twig",
    "XML"
)
domain_specific_langs <- c(
    "Vim Script", "Emacs Lisp", "Makefile",
    "Dockerfile", "Batchfile", "CSS",
    "SCSS", "Gnuplot", "Stylus",
    "Nix", "GLSL", "R",
    "ActionScript", "Adblock Filter List", "AGS Script",
    "ANTLR", "ApacheConf", "AppleScript",
    "ASL", "AutoHotkey", "AutoIt",
    "Awk", "Ballerina", "CartoCSS",
    "Chapel", "CodeQL", "Common Workflow Language",
    "Ciq", "Cuda", "Dhall",
    "DM", "Elm", "GDScript",
    "HLSL", "Inno Setup", "Lean",
    "Less", "Mathematica", "MATLAB",
    "nesC", "NSIS", "Open Policy Agent",
    "OpenQASM", "PLpgSQL", "PostScript",
    "Processing", "Prolog", "Protocol Buffer",
    "Pug", "Riot", "Sass",
    "ShaderLab", "Smali", "Solidity",
    "Starlark", "Svelte", "SVG",
    "SWIG", "SystemVerilog", "TSQL",
    "Verilog", "VHDL", "Vue",
    "XSLT", "Yacc", "YARA", "CMake"
)
# Remove repos with unrelevant primary language
lang_repos_clean <-
    lang_repos_clean[!(lang_repos_clean$language %in% domain_specific_langs), ]
lang_repos_clean <-
    lang_repos_clean[!(lang_repos_clean$language %in% markup_langs), ]

# Remove repos from languages with less than 10 occurrences
language_occurences <- table(unlist(lang_repos_clean$language))
lang_repos_clean <-
    lang_repos_clean[language_occurences[unlist(lang_repos_clean$language)] > 10, ]

# Remove repos with wrong count of issues and PRs
repos_filtered <-
    repos_filtered[repos_filtered$pull_requests > repos_filtered$bug_pull_requests, ]
repos_filtered <-
    repos_filtered[repos_filtered$issues > repos_filtered$bug_issues, ]

language_occurences <- table(unlist(repos_filtered$language)) # List of languages and their occurrences
language_occurences <- sort(language_occurences, decreasing = TRUE)
nrow(repos) # Rows before filtering
nrow(repos_filtered) # Rows after filtering
```

\newpage

# Getting language representative repositories
Now that we have identified the top used general purpose (programming) languages (GPLs) we
want to get equal amounts of data for each language to make a fair analysis.

```{r get_lang_repos, eval = FALSE}
get_lang_repos <- function(n = 10, language, key = "", results_per_page = 10) {
    language_query <- paste("language", language, sep = ":")
    return(get_repos(n, custom_search_params = language_query, key = key))
}

key <- readLines("key.secret")
lang_repos_edges <- c()
for (lang in names(language_occurences)) {
    print(paste("Getting repos of:", lang))
    lang_repos_edges <- c(lang_repos_edges, get_lang_repos(800, language = lang, key = key))
}
```

```{r lang_repos_setup, include = FALSE}
# NOTE: Loads previously gathered data from file
load("lang_repos_edges")
```


```{r format_and_clean_lang_repos, cache = TRUE}
lang_repos <- format_repo_edges(lang_repos_edges)

# Cleaning data
lang_repos_clean <- lang_repos[!duplicated(lang_repos$name), ]
lang_repos_occ <- table(unlist(lang_repos$language))
lang_repos_occ <- lang_repos_occ[lang_repos_occ > 700]
lang_repos_clean <- lang_repos_clean[lang_repos_clean$lang %in% names(lang_repos_occ), ]
```

# Results
## Top 30 000 repositories
```{r results-repos, echo = FALSE, results = 'asis'}
write.csv2(repos_filtered, file = "repos_filtered.csv")
knitr::kable(repos_filtered[1:10, ])
```

## Top general purpose programming languages
```{r results-langs, echo = FALSE, results = 'asis'}
write.csv2(language_occurences, file = "language_occurences.csv")
knitr::kable(language_occurences[language_occurences > 300],
    col.names = c("Language", "Frequency"))
```

## Repos per language of selected languages
```{r results-representative-repos}
write.csv2(lang_repos_clean, file = "lang_repos.csv")
knitr::kable(table(unlist(lang_repos_clean$language)),
    col.names = c("Language", "Frequency"))
```
