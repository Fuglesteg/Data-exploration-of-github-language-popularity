---
title: Gather github dataset
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include = FALSE}
# NOTE: Loads previously collected responses so that requests
# don't need to be run again every time.
lang_repos <- read.csv2("lang_repos_raw_100_000.csv")
```

```{r get-data, eval = FALSE, include = FALSE}

get_repos <- function(n = 10, custom_search_params = "", key = "", results_per_page = 10, body = "") {
    total_runs <- n / results_per_page
    print(paste(total_runs, " runs total."))
    library("httr")
    page_key_query <- ""
    stars_query <- 'stars:100..300000'
    repo_edges <- c()

    i <- 1
    while (i <= total_runs) {
        search_query <- paste(
            'query: "', stars_query, ' ', custom_search_params,
            '", type: REPOSITORY, first: ',
            results_per_page,
            page_key_query, sep = "")

        query <- paste(
            'query {
                search(', search_query, ') {
                    edges {
                        node {
                            ... on Repository { ',
                            body,
                            '}
                        }
                    }
                    pageInfo {
                        endCursor
                        hasNextPage
                    }
                }
            }',
            sep = '')
        request_body <- list(query = query)
        response <-
            POST("https://api.github.com/graphql", body = request_body, encode = "json",
                 add_headers(Authorization = paste("Bearer", key)))
        data <- content(response)$data
        if (is.null(data) && !is.null(content(response)$errors)) {
            print(content(response)$errors)
            print("^ Encountered error, re-running request...")
            next()
        }
        repo_edges <- append(repo_edges, data$search$edges)

        if (!data$search$pageInfo$hasNextPage) {
            print("No next page")
            print("Starting new query")
            stars_query <- paste('stars:100..',
                data$search$edges[[length(data$search$edges)]]$node$stargazerCount - 1,
                sep = '')
            print(stars_query)
            page_key_query <- ""
        } else {
            page_key_query <-
                paste(', after: "', data$search$pageInfo$endCursor, '"', sep = "")
            cat(paste("\r", i, "/", total_runs))
        }
        i <- i + 1
    }
    return(repo_edges)
}

get_repos_for_language_pop <- function(n = 10, key = "", results_per_page = 10) {
    body <- "
    stargazerCount
    nameWithOwner
    primaryLanguage {
        name
    }
    pushedAt
    createdAt
    forkCount
    isInOrganization
    repositoryTopics(first: 100) {
        nodes {
            topic {
                name
            }
        }
    }
    watchers {
        totalCount
    }
    "
    return(get_repos(n, key = key, body = body, results_per_page = results_per_page))
}

# NOTE: Loads the API key from a local file,
# this key is specific to each user and is required to run the request
key <- readLines("key.secret")
repo_lang_edges <- get_repos_for_language_pop(100000, key = key, results_per_page = 100)
```

# Getting dataset for bug analysis

Now that we have identified the top used general purpose (programming) languages (GPLs) we
want to get equal amounts of data for each language to make a fair analysis for our
second research question RQ2. We use the function defined earlier for querying the
GitHub API, but this time we run the function for each language. 
We specify the language using the "custom_search_params" parameter which let's us
use custom queries.

```{r getting_bug_data, eval = FALSE}
get_repos_for_bugs <- function(n = 10, key = "", results_per_page = 10, languages = c()) {
    repo_edges <- c()
    body <- "
    stargazerCount
    nameWithOwner
    primaryLanguage {
        name
    }
    issues {
        totalCount
    }
    pullRequests {
        totalCount
    }
    labels(first: 100) {
        totalCount
        nodes {
            name
            pullRequests {
                totalCount
            }
            issues {
                totalCount
            }
        }
    }
    "
    for (language in languages) {
        language_query <- paste("language", language, sep = ":")
        repo_edges <- c(repo_edges,
            get_repos(n,
                custom_search_params = language_query,
                key = key,
                body = body,
                results_per_page = results_per_page
            )
        )
    }
    return(repo_edges)
}

# NOTE: Loads the API key from a local file,
# this key is specific to each user and is required to run the request
key <- readLines("key.secret")
repo_lang_edges <- get_repos_for_bugs(100000, key = key,
  results_per_page = 100,
  languages = names(language_occurences)
)
```

\newpage

# Formatting the data
The data is processed into the appropriate values and transformed into a data frame.
Note that we loop over the labels again filtering on names associated with reporting bugs.

```{r format-data, eval = FALSE }
library(purrr)
format_bug_repo_edges <- function(repo_edges) {
    repos <- data.frame()
    for (edge in repo_edges) {
        repo <- edge$node
        bug_labels <-
            Filter(function(label) grepl("(bug|error|conflict|report)", label$name, ignore.case = TRUE),
                   repo$labels$nodes)
        bug_issues <- 0
        bug_pull_requests <- 0

        # Sum all issues and pull requests on every label associated with bugs
        for (label in bug_labels) {
            bug_issues <- bug_issues + label$issues$totalCount
            bug_pull_requests <- bug_pull_requests + label$pullRequests$totalCount
        }

        if (is.null(repo$primaryLanguage))
            language <- NULL
        else
            language <- repo$primaryLanguage$name

        repos <- rbind(repos,
            data.frame(name = repo$nameWithOwner,
                stars = repo$stargazerCount,
                language = I(list(language)),
                issues = repo$issues$totalCount,
                pull_requests = repo$pullRequests$totalCount,
                bug_issues = I(list(bug_issues)),
                bug_pull_requests = I(list(bug_pull_requests))))
    }
    return(repos)
}

repos <- format_lang_repo_edges(repo_lang_edges)
```

# Cleaning data
We clean the data using the techniques specified in the previous section.
However we also clean based on how many repos have been returned by the search.

```{r}
# Remove repos with less than 300 occurrences
language_occurences_bugs <- table(unlist(repos$language))
repos <-
    repos[language_occurences_bugs[unlist(repos$language)] > 300, ]

# Remove repos with wrong count of issues and PRs
repos <-
    repos[repos$pull_requests > repos$bug_pull_requests, ]
repos <-
    repos[repos$issues > repos$bug_issues, ]
```
